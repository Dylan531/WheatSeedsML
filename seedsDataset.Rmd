---
title: "Wheat Grain Dataset"
author: "Dylan, Michael, and  Johannes"
date: "4/18/2022"
output: html_document
---



## dataframes 
```{r}
set.seed(1234)
library(keras)
library(tfdatasets)

# Loading a csv file
seedsdf <- read.csv("seeds_dataset.csv")
processed_seedsdf <- read.csv("seeds_dataset.csv")

# Various syntax for R dataframes
summary(seedsdf)

# Normalize data between 1 and 2
for (i in 1:7) {
  min <- min(seedsdf[, i])
  max <- max(seedsdf[, i])
  for (j in 1:length(seedsdf[, i])) {
    processed_seedsdf[j, i] <- ((seedsdf[j, i] - min) / (max - min)) + 1
  }
}
```

## Explore the distributions of feature values using kernel density plots
If multiple distributions are visible, a particular distribution may be associated with a category. 
```{r}
for (i in 1:7) {
  plot(density(processed_seedsdf[, i]), names(processed_seedsdf)[i])
}
```

## Explore correlation among features
We should select one feature among each group of highly correlated features (>0.7?).
```{r}
# Use the R correlation function cor()
# The all.obs option assumes our data frame has no missing values. Uses Pearson
# correlation testing by default.
cor(seedsdf)
```

## Splitting data into training and testing
```{r}
sample_size <- 150
layer_units <- 128
model_loss <- "categorical_crossentropy"
model_optimizer <- "adam"
model_accuracy <- "accuracy"
model_epochs <- 75
input_shape <- 3
training_subset <- c("Area", "Asymmetry", "Compactness")
set.seed(1234) # setting random seed to make results repeatable

picked <- sample(seq_len(nrow(processed_seedsdf)), size = sample_size)
training <- seedsdf[picked, ]
testing <- seedsdf[-picked, ]


# Changing y into categorical data (performing one-hot encoding)
training$Variety <- training$Variety - 1
testing$Variety <- testing$Variety - 1
y_tr <- to_categorical(training$Variety, num_classes = 3)
y_test <- to_categorical(testing$Variety, num_classes = 3)
```

## Neural Network
```{r}
model <- keras_model_sequential() %>%
  layer_dense(
    units = layer_units, activation = "relu",
    input_shape = input_shape
  ) %>%
  layer_dense(units = layer_units, activation = "relu") %>%
  layer_dense(units = ncol(y_tr), activation = "softmax")


model %>% compile(
  loss = model_loss,
  optimizer = model_optimizer, # optimizer_rmsprop(),
  metrics = model_accuracy
)

x_tr <- as.matrix(training[, training_subset]) # need to convert to a matrix
x_test <- as.matrix(testing[, training_subset])

model %>%
  fit(
    x = x_tr, # input is the training subsets
    y = y_tr, # label is the last column
    epochs = model_epochs
  )
```

# Evaluate the model
```{r}
score <- model %>% evaluate(x_test, y_test, verbose = 0)


# Predicting likelihood of all categories:
# result <- model %>% predict(x_test)

# result

# testing[, 8]

# Table Data
variable_content <- c(
  sample_size, layer_units, model_loss, model_optimizer,
  model_accuracy, model_epochs, input_shape, toString(training_subset),
  round(score[1], 4), round(score[2], 4)
)
data <- matrix(variable_content, ncol = 10, byrow = TRUE)

#  Table labels
colnames(data) <- c(
  "Sample Size", "Layer Units", "Loss", "Optimizer",
  "Accuracy", "Epochs", "Input Shape", "Feature Selection",
  "Test Loss", "Test Accuracy"
)
rownames(data) <- c("")
table <- as.table(data)

# Display table
table

# Append run info to testData.csv
write.table(data,
  file = "experiment_history.csv", sep = ",", append = T,
  row.names = F, col.names = F
)
```
