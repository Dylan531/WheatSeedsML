---
title: "seedsDataset"
author: "Dylan, Michael, and  Johannes"
date: "4/18/2022"
output: html_document
---



## dataframes 
```{r}
set.seed(1234)
library(keras)
library(tfdatasets)

#Loading a csv file
seedsdf <- read.csv("seeds_dataset.csv")
seedsdfProcessed <- read.csv("seeds_dataset.csv")

# Various syntax for R dataframes
summary(seedsdf)


# Normalize data between 1 and 2
 for (i in 1:7) {
  min <- min(seedsdf[,i])
  max <- max(seedsdf[,i])
  for (j in 1:length(seedsdf[,i])) { 
    seedsdfProcessed[j,i] <- ((seedsdf[j,i]-min)/(max-min))+1
    print(seedsdfProcessed[j,i])
  }
}
```
## Explore the distributions of feature values using kernel density plots
If multiple distributions are visible, a particular distribution may be associated with a category. 
```{r}
for (i in 1:7) {
  plot(density(seedsdfProcessed[,i]))
}

```
## Explore correlation among features
We should select one feature among each group of highly correlated features (>0.7?).
```{r}
# Use the R correlation function cor()
# The all.obs option assumes our data frame has no missing values. Uses Pearson correlation testing by default.
cor(seedsdf)

```

## Splitting data into training and testing

```{r}
sample_size <- 150
layer.units <- 128
model.loss <- "categorical_crossentropy"
model.optimizer <- "adam"
model.accuracy <- "accuracy"
model.epochs <- 75
input.shape <- 3
training.subset <- c("area","asymmetry.cof","compactness")
set.seed(1234) # setting random seed to make results repeatable

picked <- sample(seq_len(nrow(seedsdfProcessed)),size = sample_size)
training <- seedsdf[picked,]
testing <- seedsdf[-picked,]


# Changing y into categorical data (performing one-hot encoding)
training$variety <- training$variety-1
testing$variety <- testing$variety-1
yTr <- to_categorical(training$variety, num_classes = 3)
yTest <- to_categorical(testing$variety, num_classes = 3)

```

## Neural network for the iris example

```{r}
model = keras_model_sequential() %>%
  layer_dense(units = layer.units, activation = "relu",input_shape=input.shape) %>%
  layer_dense(units = layer.units, activation = "relu") %>%
  layer_dense(units = ncol(yTr), activation = "softmax")


model %>% compile(
  loss = model.loss,
  optimizer = model.optimizer, #optimizer_rmsprop(),
  metrics = model.accuracy
)

xTr <- as.matrix(training[,training.subset]) # need to convert to a matrix
xTest <- as.matrix(testing[,training.subset])

model %>% 
  fit(
    x = xTr, # input is the training subsets
    y = yTr, # label is the last column
    epochs = model.epochs
  )

```

# Evaluate the model
```{r}
score = model %>% evaluate(xTest, yTest, verbose = 0)


# Predicting likelihood of all categories:
result <- model %>% predict(xTest)

result

testing[,8]

# Print table
variable.content <- c(sample_size, layer.units, model.loss, model.optimizer, model.accuracy, model.epochs, input.shape, toString(training.subset), round(score[1], 4), round(score[2], 4))
data = matrix(variable.content, ncol=10, byrow=TRUE)

# Add column names
colnames(data) = c('Sample Size','Layer Units','Loss','Optimizer', 'Accuracy', 'Epochs', 'Input Shape', 'Feature Selection', 'Test Loss', 'Test Accuracy')
rownames(data) = c('')
table=as.table(data)

# display
table

write.table(data,file="testData.csv", sep = ",", append = T, row.names=F, col.names=F)
```
